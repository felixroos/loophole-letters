---
title: Trying FAUST on the Web
description: ""
date: "2023-09-06"
tags: ["code"]
draft: true
---

Let's play with FAUST on the web. The repo [@grame/faustwasm](https://www.npmjs.com/package/@grame/faustwasm) has different export options. I chose:

```sh
node scripts/faust2wasm.js test/rev.dsp test/out -standalone
```

This generates a html page with a UI to control a reverb effect + mic input. It loads `dspModule.wasm` for the audio processing.

It might be really handy if the package.json of faustwasm had a `bin` field, which would allow using the tool without cloning the repo, just by using npx.

## Loading FAUST with Web Audio

I was able to condense a quick setup to this:

```js
import dspModuleUrl from "./reverb/dspModule.wasm?url";
import dspMeta from "./reverb/dspMeta.json";
import { FaustMonoDspGenerator } from "@grame/faustwasm";

const loadFaustReverb = async (ac) => {
  const module = await WebAssembly.compileStreaming(fetch(dspModuleUrl));
  const generator = new FaustMonoDspGenerator();
  const node = await generator.createNode(ac, "FaustMonoDSP", {
    module,
    json: JSON.stringify(dspMeta),
  });
  node.connect(ac.destination);
  return node;
};
```

(This will only work on vite, but replacing the dspModule path with a public path should also work anywhere else)

To test the reverb, let's load a test sample:

```js
async function loadClapBuffer(ac) {
  const res = await fetch(
    "https://raw.githubusercontent.com/tidalcycles/Dirt-Samples/master/cp/HANDCLP0.wav"
  );
  const buffer = await res.arrayBuffer();
  const audioBuffer = await ac.decodeAudioData(buffer);
  return audioBuffer;
}
```

Now let's connect it together:

```js
async function play() {
  const ac = new AudioContext();
  const reverb = await loadFaustReverb(ac);

  const source = ac.createBufferSource();
  source.buffer = await loadClapBuffer(ac);
  source.connect(reverb);
  source.start();
}
```

import { FaustReverb } from "../../components/faust/Faust.jsx";

This is how it sounds like:

<FaustReverb client:only />

The next question would be how the parameters can be controlled from the outside..

## Working With Params

This is the API to work with params:

```js
const reverb = await loadFaustReverb(ac);
const params = reverb.getParams();
// ['/Freeverb/0x00/Damp', '/Freeverb/0x00/RoomSize', '/Freeverb/0x00/Stereo_Spread', '/Freeverb/Wet']
const damp = reverb.getParamValue("/Freeverb/0x00/Damp"); // 0.5
reverb.setParamValue("/Freeverb/0x00/Damp", 0.75);
```

We could convert params to an object like this:

```js
const getParams = (faustNode) =>
  Object.fromEntries(
    faustNode.getParams().map((key) => [key, faustNode.getParamValue(key)])
  );
console.log(getParams(reverb));
/* 
{ 
  '/Freeverb/0x00/Damp': 0.5, 
  '/Freeverb/0x00/RoomSize': 0.5, 
  '/Freeverb/0x00/Stereo_Spread': 0.5, 
  '/Freeverb/Wet': 0.33 
}
*/
```

... which should be enough to spin up a UI!

## PolySynth

FAUST also has a poly mode, where a single module can handle multiple voices.
Taking this totally cheap "synth":

```plaintext
import("stdfaust.lib");
f = hslider("freq", 440, 50, 2000, 1);
g = hslider("gain", 0.5, 0, 1, .01);
t = button("gate");
process = os.osc(f) * t * g;
```

This follows the `freq` / `gate` / `gain` convention, which is expected for polyphonic modules. We can generate it with:

```sh
node scripts/faust2wasm.js test/synth.dsp test/out -poly
```

for some reason, this logs an error:

```sh
Faust Compiler version: 2.63.0
Reading file test/synth.dsp
Error: synth : 10 : ERROR : undefined symbol : effect

    at _FaustCompiler.createDSPFactory (file:///Users/felix/projects/faustwasm/dist/esm/index.js:1614:36)
    at async _FaustPolyDspGenerator.compile (file:///Users/felix/projects/faustwasm/dist/esm/index.js:3648:28)
    at async faust2wasmFiles (file:///Users/felix/projects/faustwasm/src/faust2wasmFiles.js:50:21)
    at async file:///Users/felix/projects/faustwasm/scripts/faust2wasm.js:27:5
Compilation successful (29 ms).
Writing files to test/out
```

But anyway, it generates `dspModule.wasm` as well as `mixerModule.wasm`, which seems fine.

To know how these files can be used in poly mode, I took inspiration from the standalone code, which can be condensed to:

```js
import dspModuleUrl from "./synth/dspModule.wasm?url";
import mixerModuleUrl from "./synth/mixerModule.wasm?url";
import dspMeta from "./synth/dspMeta.json";
import { FaustPolyDspGenerator } from "@grame/faustwasm";

const loadFaustSynth = async (ac) => {
  const [module, mixerModule] = await Promise.all([
    WebAssembly.compileStreaming(fetch(dspModuleUrl)),
    WebAssembly.compileStreaming(fetch(mixerModuleUrl)),
  ]);
  const generator = new FaustPolyDspGenerator();
  const node = await generator.createNode(
    ac,
    16, // voices
    "FaustPolyDSP",
    {
      module,
      json: JSON.stringify(dspMeta),
    },
    mixerModule
  );
  node.connect(ac.destination);
  return node;
};
```

This is pretty similar to how it works in mono mode, just passing the mixerModule + specifying the number of voices.

In the the standalone app, voices are played using the `midiMessage` method like this:

```js
const midiAccess = await navigator.requestMIDIAccess();
const { inputs } = midiAccess;
const currentInput = midiAccess.inputs[0];
currentInput.addEventListener("midimessage", () => {
  (e) => faustNode.midiMessage(e.data);
});
```

If a midi device is connected, note events will now play the little synth!
To simplify testing without a midi device, let's just hardcode something that looks like a midi event:

```js
synth.midiMessage(new Uint8Array([144, 56, 114]));
synth.midiMessage(new Uint8Array([144, 59, 114]));
setTimeout(() => {
  synth.midiMessage(new Uint8Array([144, 56, 0]));
  synth.midiMessage(new Uint8Array([144, 59, 0]));
}, 500);
```

This will just play 2 notes simultaneously of 500ms... Here it is:

import { FaustSynth } from "../../components/faust/FaustSynth.jsx";

<FaustSynth client:only />

While this is certainly a great and easy to use feature, it is not enough for what I am looking for,
which is not only controlling frequency / gate / gain, but also any other param in a polyphonic fashion.
So for example, if there was a filter, each voice should potentially have a different cutoff value.

Looking closer at `FaustAudioWorkletNode.ts`, it looks like the `midiMessage` method sends the midi date via postmessage of type `midi`.

The `FaustAudioWorkletProcessor` then takes these postMessages and calls methods on `fDSPCode`, for example `fDSPCode.midiMessage`.
In the end it allocates a `FaustWebAudioDspVoice` and calls `keyOn` / `keyOff`, passing `pitch` and `velocity`.
What gives hope is that a `FaustWebAudioDspVoice` also has `setParamValue` / `getParamValue` methods, which look like you could set params on an individual voice.

Here are all relevant parts of the code:

```js
class FaustPolyWebAudioDsp extends FaustBaseWebAudioDsp {
  /* stuff */
  // private fVoiceTable: FaustWebAudioDspVoice[]; // <- allocated voices
  midiMessage(data: number[] | Uint8Array) {
    /* stuff */
    if (cmd === 8 || (cmd === 9 && data2 === 0))
      return this.keyOff(channel, data1, data2);
    else if (cmd === 9) return this.keyOn(channel, data1, data2);
    else super.midiMessage(data); // cc message
  }
  keyOn(channel: number, pitch: number, velocity: number) {
    /* stuff */
    const voice = this.getFreeVoice(); // sets this.fVoiceTable[voice]
    this.fVoiceTable[voice].keyOn(
      pitch,
      velocity,
      this.fVoiceTable[voice].fCurNote == FaustWebAudioDspVoice.kLegatoVoice
    );
  }
}

class FaustWebAudioDspVoice {
  // private fAPI: IFaustDspInstance;
  keyOn(pitch: number, velocity: number /* */) {
    this.fFreqLabel.forEach((index) =>
      this.fAPI.setParamValue(
        this.fDSP,
        index,
        FaustWebAudioDspVoice.midiToFreq(pitch)
      )
    );
    this.fGateLabel.forEach((index) =>
      this.fAPI.setParamValue(this.fDSP, index, 1)
    );
    this.fGainLabel.forEach((index) =>
      this.fAPI.setParamValue(
        this.fDSP,
        index,
        FaustWebAudioDspVoice.normalizeVelocity(velocity)
      )
    );
    // Keep pitch
    this.fCurNote = pitch;
  }
}
```

So theoretically, `FaustPolyWebAudioDsp` could have a similar method to `keyOn`, with the ability to set arbitrary key value pairs.
Similarly, `FaustWebAudioDspVoice` would then have an equivalent method to its `keyOn`, where all key value pairs are converted to `setParamValue` calls.
Ideally, it should just ignore keys that are not present in the dsp module and just use the ones that are known...

If that approach won't work, full polyphony of effects could also be achieved by just using multiple mono nodes.

Another potential roadblock lies in the fact that nodes cannot be started at a precise point in time.
In the Web Audio API [AudioScheduledSourceNode](https://developer.mozilla.org/en-US/docs/Web/API/AudioScheduledSourceNode) have `start` and `stop` methods that take an audio context time.
Without the ability to to that, events cannot be scheduled ahead of time.

Questions:

- Is it bad for performance if the voices are set really high, without using them?
- is the poly mode just spawning multiple audioworklets from the outside, or is it a feature of the native part?
- is there a way to schedule `start` and `stop` or `setParamValue` at a specific audio context time?

## FAUST Compiler

There is also a lean API to compile DSP on the fly:

```js
import {
  instantiateFaustModuleFromFile,
  LibFaust,
  FaustMonoDspGenerator,
  FaustCompiler,
} from "@grame/faustwasm";
import libfaustwasm from "@grame/faustwasm/libfaust-wasm/libfaust-wasm.js?url";
import libfaustwasmdata from "@grame/faustwasm/libfaust-wasm/libfaust-wasm.data?url";
import libfaustwasmwasm from "@grame/faustwasm/libfaust-wasm/libfaust-wasm.wasm?url";

async function loadCompiler() {
  const faustModule = await instantiateFaustModuleFromFile(
    libfaustwasm,
    libfaustwasmdata,
    libfaustwasmwasm
  );
  const libFaust = new LibFaust(faustModule);
  return new FaustCompiler(libFaust);
}

let compiler, generator, ac, node;

async function compile(code) {
  compiler = compiler || (await loadCompiler());
  ac = ac || new AudioContext();
  generator = generator || new FaustMonoDspGenerator();

  const name = "FaustCompile";
  const argv = ["-I", "libraries/"];
  const compiled = await generator.compile(
    compiler,
    name,
    code,
    argv.join(" ")
  );

  return generator.createNode(ac, "FaustMonoDSP", compiled.factory);
}
const compileAndRun = async (code) => {
  node?.stop();
  node = await compile(code);
  node.connect(ac.destination);
};
```

Throwing this into a component gives instant live codability:

import { FaustCompile } from "../../components/faust/FaustCompile.jsx";

<FaustCompile
  value={`import("stdfaust.lib");
process = ba.pulsen(1, 10000) : pm.djembe(60, 0.3, 0.4, 1);`}
  client:only
/>

You can also evaluate with ctrl+enter and stop with ctrl+.

## faust2wam

With faust2wam, webaudiomodules can be created from a faust dsp file:

```js
node faust2wam.js test/synth.dsp test/out -poly
```

Looking at the generated code, the essential bit is this:

```js
class FaustCompositeAudioNode extends CompositeAudioNode {
  setup(output, paramMgr) {
    if (output.numberOfInputs > 0) this.connect(output, 0, 0);
    paramMgr.addEventListener("wam-midi", (e) =>
      output.midiMessage(e.detail.data.bytes)
    );
    this._wamNode = paramMgr;
    this._output = output;
  }

  destroy() {
    super.destroy();
    if (this._output) this._output.destroy();
  }

  getParamValue(name) {
    return this._wamNode.getParamValue(name);
  }

  setParamValue(name, value) {
    return this._wamNode.setParamValue(name, value);
  }
}

const getBasetUrl = (relativeURL) => {
  const baseURL = relativeURL.href.substring(
    0,
    relativeURL.href.lastIndexOf("/")
  );
  return baseURL;
};

export default class FaustPlugin extends WebAudioModule {
  _PluginFactory;
  _baseURL = getBasetUrl(new URL(".", import.meta.url));
  _descriptorUrl = `${this._baseURL}/descriptor.json`;

  async initialize(state) {
    await this._loadDescriptor();
    return super.initialize(state);
  }

  async createAudioNode(initialState) {
    const faustNode = await loadFaustSynth(this.audioContext);
    const paramMgrNode = await ParamMgrFactory.create(this, {
      internalParamsConfig: Object.fromEntries(faustNode.parameters),
    });
    const node = new FaustCompositeAudioNode(this.audioContext);
    node.setup(faustNode, paramMgrNode);
    if (initialState) node.setState(initialState);
    return node;
  }

  createGui() {
    console.log("i dont care about gui");
  }
}
```

A very simple host can be implemented to load and play it:

```js
export function WamFaust() {
  return (
    <button
      onClick={async () => {
        const ac = new AudioContext();
        const { initializeWamHost } = await import("@webaudiomodules/sdk");
        const [hostGroupId] = await initializeWamHost(ac);

        const { default: pluginFactory } = await import(
          "./faust-synth/index.js"
        );
        const instance = await pluginFactory.createInstance(
          hostGroupId,
          ac,
          {}
        );
        instance.audioNode.connect(ac.destination);
        const synth = instance.audioNode._output;

        synth.midiMessage(new Uint8Array([144, 56, 114]));
        synth.midiMessage(new Uint8Array([144, 59, 114]));
        setTimeout(() => {
          synth.midiMessage(new Uint8Array([144, 56, 0]));
          synth.midiMessage(new Uint8Array([144, 59, 0]));
        }, 500);
      }}
    >
      GO
    </button>
  );
}
```

import { WamFaust } from "../../components/wam/WamFaust.jsx";

<WamFaust client:only />
